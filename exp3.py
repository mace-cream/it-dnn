#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Experiment in Section 6.3
H-score for Mixture Gaussian Data
"""

import numpy as np
import matplotlib.pyplot as plt

from keras.models import Sequential, Model
from keras.layers import Dense, Activation
from keras.optimizers import SGD

from func import h_score, MakeLabels
plt.rc('font', family='serif')
plt.ion()

def GenerateSamples(xDim, yCard, nPerClass, nSamples):
    """
    Generate samples of data (X, Y).
    X is a vector with dimension xDim, Y is corresponding discrete label taking value from {0, 1, 2, ..., yCard - 1}
    The distribution of X|Y=i is a mixture Gaussian with nPerClass mixtures.
    Input args:
        xDim: dimension of X
        yCard: Cardinality of Y
        nPerClass: # of mixtures within each class
        nSamples: # of points generated

    return: [X, Y]
        X: n samples with dimension xDim
        Y: Corresponding Labels
    """
    u1 = np.random.choice(yCard, nSamples) # u1 indicates the label of class
    u2 = np.random.choice(nPerClass, nSamples) # u2 indicates the id of mixture in each class
    """ Centers of these mixtures are also generated by Gaussian distribution  """
    mu_c = 0 # mean of center
    sigma_c = 1 # deviation of center
    center = np.random.normal(mu_c, sigma_c, (yCard * nPerClass, xDim))
    mu_x = 0 # mean of each gaussin
    sigma_x = .2 # deviation of each gaussian
    X = np.random.normal(mu_x, sigma_x, (nSamples, xDim)) + center[u1 * nPerClass + u2, :]
    Y = u1    
    return([X, Y])

nSamples = 100000

nPerClass = 6
# If nPerClass is large (like 20, 30), the h-score and accuracy would all be low, but we can still see the mononically increasing trends of h-score over layers
yCard = 6
# too lazy to find more colors in plotting, so yCard needs to be no greater than 7. If yCard > 7, you need to expand color_list
dim_list = [6, 14, 12, 10, 8, 6] # Dimension list of NN, could be modified accordingly
xDim = dim_list[0] # Making xDim larger leads to easier separating -- Bless of dimensionality :) 
[X, Y] = GenerateSamples(xDim, yCard, nPerClass, nSamples)
plt.figure()
color_list = np.array(['r', 'g', 'b', 'c', 'm', 'k', 'y']) # colors for each class
plt.scatter(X[:, 0], X[:, 1], s = 1, c = color_list[[int(y) for y in Y]]) # plot the first 2 dimensions of X with Y indicated by different colors. 
plt.axis('equal')
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.draw()

YLabels = MakeLabels(Y)

model = Sequential()

for id in range(len(dim_list) - 1):
    model.add(Dense(dim_list[id + 1], activation = 'relu', input_dim=dim_list[id]))
model.add(Dense(yCard, activation='softmax', input_dim=dim_list[-1]))

sgd = SGD(.2, decay=1e-6, momentum=0.9, nesterov=True, clipvalue=0.04)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])
# nEpochs = 120
plt.figure() # Plot the trends of H-score over layers during the training process 
t_model = Model(inputs=model.input, outputs=[layer.input for layer in model.layers])
fmt_list = ['-', ':', '--', '-.', 'x-']
# t_model is used to get outputs from each layer.
s_list = t_model.predict(X) # list of outputs from all layers
h_score_list = [h_score(s, Y) for s in s_list]
acc = model.evaluate(X, YLabels, verbose = 0)[1]
plt.plot(range(len(h_score_list)), h_score_list, fmt_list[0], label='0 epochs' + ', Acc = ' + f"{acc:.3f}", linewidth = 3)
epoch_list = [0, 20, 40, 80, 160]
for i in range(len(epoch_list) - 1):
    hist = model.fit(X, YLabels, verbose=0, batch_size=nSamples, epochs=epoch_list[i + 1] - epoch_list[i])
    t_model = Model(inputs=model.input, outputs=[layer.input for layer in model.layers])
    # t_model is used to get outputs from each layer.
    s_list = t_model.predict(X) # list of outputs from all layers
    h_score_list = [h_score(s, Y) for s in s_list]
    acc = model.evaluate(X, YLabels, verbose = 0)[1]
    epochs = epoch_list[i+1]
    plt.plot(range(len(h_score_list)), h_score_list, fmt_list[i + 1], label= f"{epochs:3d}" + ' epochs' + ', Acc = ' + f"{acc:.3f}", linewidth = 3, markersize = 12)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
plt.legend(loc='best', fontsize = 16)

# Ignore the last layer.
new_model = Sequential()
layer_num = -2
new_model.add(Dense(yCard, activation = 'softmax', input_dim = dim_list[layer_num]))
new_model.compile(loss='categorical_crossentropy',
                  optimizer=sgd,
                  metrics=['accuracy'])
new_model.fit(s_list[layer_num], YLabels, batch_size=nSamples, epochs = 100, verbose=0)
new_acc = new_model.evaluate(s_list[layer_num], YLabels, verbose = 0)[1]
# Accuracy of linear classification using the featue extracted by the 5th layer
print(new_acc)
plt.draw()
